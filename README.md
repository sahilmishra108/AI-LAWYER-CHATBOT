# ğŸ§  AI-Lawyer Chatbot â€“ Legal Question Answering using RAG & DeepSeek R1

The **AI-Lawyer Chatbot** is an intelligent legal assistant that uses **Retrieval-Augmented Generation (RAG)** to answer legal questions based on uploaded PDF documents. Powered by the **DeepSeek R1** model and the **LangChain** framework, it combines document retrieval and large language models to generate grounded and context-aware responses.

## ğŸ“Œ Key Features

- Upload any legal or human rights PDF
- Context-aware legal Q&A using DeepSeek R1 via Groq
- Semantic document retrieval using FAISS and Ollama embeddings
- Built using LangChain, Streamlit, and Ollama
- Reduces hallucination and ensures context fidelity via RAG

## ğŸ“‚ Project Structure
| File/Folder                              | Description                                      |
|------------------------------------------|--------------------------------------------------|
| ğŸ“ `AI-Lawyer-Chatbot/`                  | Root project directory                           |
| â”œâ”€â”€ `main.py`                            | End-to-end chatbot application                   |
| â”œâ”€â”€ `frontend.py`                        | Minimal UI version                               |
| â”œâ”€â”€ `rag_pipeline.py`                   | LLM + Retrieval + Prompt engineering             |
| â”œâ”€â”€ `vector_database.py`                | Vector DB creation using FAISS                   |
| â”œâ”€â”€ `universal_declaration_of_human_rights.pdf` | Example legal document                    |
| â”œâ”€â”€ `requirements.txt`                  | Dependency list (for pip users)                  |
| â”œâ”€â”€ `Pipfile` / `Pipfile.lock`          | Pipenv environment specification                 |
| â”œâ”€â”€ `Readme.md`                         | Project documentation                            |
| â””â”€â”€ `vectorstore/`                      | Folder to store FAISS vector index               |


## ğŸ§± Core Technologies

- **DeepSeek R1** â€“ Language model for legal reasoning (used via Groq and Ollama)
- **LangChain** â€“ Framework to build and manage the RAG pipeline
- **FAISS** â€“ Efficient vector similarity search
- **Ollama** â€“ Embedding generation using locally hosted LLMs
- **Streamlit** â€“ For building the chatbot user interface
- **PDFPlumber** â€“ Extracts and loads PDF text content

## ğŸ“ Architecture

### Phase 1 â€“ User Interface (Streamlit)

- Upload PDF
- Enter question

### Phase 2 â€“ Vector Store Setup

- PDF loading
- Chunking (RecursiveCharacterTextSplitter)
- Embedding with DeepSeek via Ollama
- Store vectors in FAISS

### Phase 3 â€“ RAG Inference Pipeline

- Retrieve relevant chunks from FAISS
- Answer using DeepSeek R1 with Groq
  ![Screenshot 2025-06-08 001001](https://github.com/user-attachments/assets/d23e6f18-9a2c-4afa-b57f-964a4d4463fc)


## âš™ï¸ Setup Guide (Quick Start)

### ğŸ”¹ Option 1: Pipenv (Recommended)

```bash
pip install pipenv
pipenv install
pipenv shell
```
ğŸ”¹ Option 2: pip + virtualenv
```bash
pip install virtualenv
virtualenv venv

# Activate the environment
# On Windows:
venv\Scripts\activate

# On macOS/Linux:
source venv/bin/activate

pip install -r requirements.txt
```
ğŸ”¹ Option 3: Conda

```bash
conda create -n ai-lawyer python=3.13
conda activate ai-lawyer
pip install -r requirements.txt
```

## ğŸš€ Run the Application

### ğŸ“¦ Full Pipeline

```bash
streamlit run main.py
```
## â–¶ï¸ Run Components Individually

### ğŸ§© Frontend Only

```bash
streamlit run frontend.py
```
### ğŸ—ƒï¸ Vector DB Setup

```bash
python vector_database.py
```
### ğŸ§  Run RAG Pipeline

```bash
python rag_pipeline.py
```
![Screenshot 2025-06-19 191440](https://github.com/user-attachments/assets/8d7a9139-1f79-4f74-9610-1ed1d1da2185)

## ğŸ§ª Example Use Case

Upload `universal_declaration_of_human_rights.pdf` and ask:


If a government forbids the right to assemble peacefully, which articles are violated?

The chatbot will:

ğŸ” Search for relevant sections in the PDF

ğŸ§  Construct a context-aware prompt

ğŸ’¬ Generate a grounded legal explanation using DeepSeek R1
  

ğŸ“˜ What is RAG?
---
Retrieval-Augmented Generation (RAG) enhances LLMs by integrating relevant external documents. It helps:

ğŸ¯ Anchor responses in real data

âš–ï¸ Improve factual accuracy

ğŸš« Reduce hallucinations

RAG Process:
Vectorize and store documents

Retrieve semantically relevant content

Generate answers using retrieved context

## ğŸ“š References

- [LangChain](https://www.langchain.com/)
- [Ollama](https://ollama.com/)
- [Groq](https://groq.com/)
- [DeepSeek R1](https://deepseek.com/)
- [Streamlit](https://streamlit.io/)
- [FAISS](https://github.com/facebookresearch/faiss)
---
  ğŸ“„ License
  ---
MIT License â€“ feel free to use, modify, and distribute with credit.


  












 
